{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from net import Net\n",
    "from data_generator import DataGenerator\n",
    "from datetime import datetime\n",
    "from tensorflow.compat.v1.data import Iterator\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "train_file = 'tables/train.txt'\n",
    "val_file = 'tables/val.txt'\n",
    "\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "dropout_rate = 0.5\n",
    "num_classes = 2\n",
    "train_layers = ['fc8', 'fc7', 'fc6']\n",
    "\n",
    "display_step = 20\n",
    "\n",
    "filewriter_path = \"tmp/tensorboard\"\n",
    "checkpoint_path = \"tmp/checkpoints\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.mkdir('tmp/')\n",
    "    os.mkdir('tmp/checkpoints')\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        tr_data = DataGenerator(train_file,\n",
    "                                mode='training',\n",
    "                                batch_size=batch_size,\n",
    "                                num_classes=num_classes,\n",
    "                                shuffle=True)\n",
    "        val_data = DataGenerator(val_file,\n",
    "                                 mode='inference',\n",
    "                                 batch_size=batch_size,\n",
    "                                 num_classes=num_classes,\n",
    "                                 shuffle=False)\n",
    "\n",
    "        iterator = Iterator.from_structure(tr_data.data.output_types,\n",
    "                                           tr_data.data.output_shapes)\n",
    "        next_batch = iterator.get_next()\n",
    "\n",
    "    training_init_op = iterator.make_initializer(tr_data.data)\n",
    "    validation_init_op = iterator.make_initializer(val_data.data)\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [batch_size, 227, 227, 3])\n",
    "    y = tf.placeholder(tf.float32, [batch_size, num_classes])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    model = Net(x, keep_prob, num_classes, train_layers)\n",
    "\n",
    "    score = model.fc8\n",
    "\n",
    "    var_list = [v for v in tf.trainable_variables() if v.name.split('/')[0] in train_layers]\n",
    "\n",
    "    with tf.name_scope(\"cross_ent\"):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=score, labels=y))\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        gradients = tf.gradients(loss, var_list)\n",
    "        gradients = list(zip(gradients, var_list))\n",
    "\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars=gradients)\n",
    "\n",
    "    for gradient, var in gradients:\n",
    "        tf.summary.histogram(var.name + '/gradient', gradient)\n",
    "\n",
    "    for var in var_list:\n",
    "        tf.summary.histogram(var.name, var)\n",
    "\n",
    "    tf.summary.scalar('cross_entropy', loss)\n",
    "\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_pred = tf.equal(tf.argmax(score, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "\n",
    "    writer = tf.summary.FileWriter(filewriter_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    train_batches_per_epoch = int(np.floor(tr_data.data_size/batch_size))\n",
    "    val_batches_per_epoch = int(np.floor(val_data.data_size / batch_size))\n",
    "\n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        writer.add_graph(sess.graph)\n",
    "\n",
    "        model.load_weights(sess)\n",
    "\n",
    "        print(f'{datetime.now()} Start training...')\n",
    "        print(f'{datetime.now()} Open Tensorboard at --logdir {filewriter_path}')\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'{datetime.now()} Epoch number: {epoch+1}')\n",
    "\n",
    "            sess.run(training_init_op)\n",
    "            for step in range(train_batches_per_epoch):\n",
    "\n",
    "                img_batch, label_batch = sess.run(next_batch)\n",
    "                sess.run(train_op, feed_dict={x: img_batch,\n",
    "                                              y: label_batch,\n",
    "                                              keep_prob: dropout_rate})\n",
    "                if step % display_step == 0:\n",
    "                    s = sess.run(merged_summary, feed_dict={x: img_batch, y: label_batch, keep_prob: 1.})\n",
    "                    writer.add_summary(s, epoch*train_batches_per_epoch + step)\n",
    "\n",
    "            print(f'{datetime.now()} Start validation')\n",
    "            sess.run(validation_init_op)\n",
    "            test_acc = 0.\n",
    "            test_count = 0\n",
    "            for _ in range(val_batches_per_epoch):\n",
    "                img_batch, label_batch = sess.run(next_batch)\n",
    "                acc = sess.run(accuracy, feed_dict={x: img_batch, y: label_batch, keep_prob: 1.})\n",
    "                test_acc += acc\n",
    "                test_count += 1\n",
    "            test_acc /= test_count\n",
    "            print(f'{datetime.now()} Validation Accuracy = {test_acc :.4f}')\n",
    "            print(f'{datetime.now()} Saving checkpoint of model...')\n",
    "\n",
    "            checkpoint_name = os.path.join(checkpoint_path, 'model_epoch'+str(epoch+1)+'.ckpt')\n",
    "            save_path = saver.save(sess, checkpoint_name)\n",
    "\n",
    "            print(f'{datetime.now()} Model checkpoint saved at {checkpoint_name}')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
